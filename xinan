import pandas as pd
import numpy as np
import lightgbm as lgb
import seaborn as sns
import keras
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss
from sklearn.preprocessing import LabelEncoder
import datetime
from sklearn.feature_selection import chi2, SelectPercentile
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import CountVectorizer
from scipy import sparse
import lightgbm as lgb
import warnings
import time
import pandas as pd
import numpy as np
import os

from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.cross_validation import StratifiedKFold
from sklearn.metrics import roc_auc_score, log_loss
import time

from sklearn.linear_model import LogisticRegression
import datetime
import gc
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.cross_validation import StratifiedKFold
from sklearn.metrics import roc_auc_score, log_loss
import lightgbm as lgb
import xgboost as xgb
import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelBinarizer
from sklearn import preprocessing
from datetime import datetime
from sklearn.preprocessing import LabelEncoder,OneHotEncoder

from sklearn.linear_model import LinearRegression
from sklearn import metrics
le = preprocessing.LabelEncoder()




train=pd.read_csv(r'C:\Users\Administrator\Desktop\da\train_xy.csv')
test=pd.read_csv(r'C:\Users\Administrator\Desktop\da\test_all.csv')
test['y']=-1
drop=['x_110','x_112','x_116','x_129','x_132','x_134','cust_group']
train.drop(drop,axis=1,inplace=True)
test.drop(drop,axis=1,inplace=True)
predict_result=pd.DataFrame()
predict_result['cust_id']=test.cust_id.values
predict_result['prob' ]=0
lable=train.y
data=pd.concat([train,test],axis=0,ignore_index=True)
data.drop('cust_id',axis=1,inplace=True)

#he=train.head()



#data['cust_group'] = data['cust_group'].map(lambda x: x.lstrip('group_').rstrip('aAbBcC')).astype(int)

shu=data.nunique()
da=data[data.y==1]
sh=da.nunique()
c=data.columns
train=data[data.y!=-1]
train.drop('y',axis=1,inplace=True)
train=train.values
#t1=train[train.cust_group==1]
#t2=train[train.cust_group==2]
#t3=train[train.cust_group==3]
test=data[data.y==-1]
test.drop('y',axis=1,inplace=True)
test=test.values
model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.01, n_estimators=5000,
                           max_bin=425, subsample_for_bin=50000, objective='binary', min_split_gain=0,
                           min_child_weight=5, min_child_samples=10, subsample=0.8, subsample_freq=1,
                           colsample_bytree=1, reg_alpha=3, reg_lambda=5, seed=1000, n_jobs=10, silent=True)

# 8折交叉训练，构造五个模型
skf=list(StratifiedKFold(lable, n_folds=5, shuffle=True, random_state=0))
baseloss = []
loss = 0


    
for i, (train_index, test_index) in enumerate(skf):
    print("Fold", i)
    lgb_model = model.fit(train[train_index], lable[train_index],
                          eval_names =['train','valid'],
                          eval_metric='auc',
                          eval_set=[(train[train_index], lable[train_index]), 
                                    (train[test_index], lable[test_index])],early_stopping_rounds=100)
    baseloss.append(lgb_model.best_score_['valid']['auc'])
    loss += lgb_model.best_score_['valid']['auc']
    train_p=lgb_model.predict_proba(train, num_iteration=lgb_model.best_iteration_)[:, 1]
    test_pred= lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]
    print('test mean:', test_pred.mean())
   
    predict_result['prob']  =predict_result['prob'] + test_pred
print('logloss:', baseloss, loss)
predict_result['predicted_score'] = predict_result['prob'] / 5



predict_result[['cust_id', 'predicted_score']].to_csv(r'C:\Users\Administrator\Desktop\da/submission.csv'  , index=False)
